{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBgvzHMnBXE1"
      },
      "source": [
        "# Automatic Speech Recognition (ASR)\n",
        "The **Automatic Speech Recognition (ASR)** *pipeline in Hugging Face's Transformers* library provides a streamlined way *to convert spoken audio into text.*\n",
        "\n",
        "This pipeline abstracts away --\n",
        "- the complexities of model loading,\n",
        "- pre-processing, and\n",
        "- post-processing,\n",
        "- allowing for quick and efficient **ASR** inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nyTEWw1CWbj"
      },
      "source": [
        "## The ASR pipeline typically involves the following stages:\n",
        "- **Feature Extraction:**\n",
        "\n",
        "  The **raw audio input is pre-processed** by a feature extractor to convert it into a **numerical representation** suitable for the model, often **log-mel spectrograms.**\n",
        "\n",
        "- **Model Inference:**\n",
        "  \n",
        "  A **pre-trained ASR** model then takes these features and performs the **sequence-to-sequence mapping**, predicting a sequence of tokens.\n",
        "\n",
        "- **Tokenization and Post-processing:**\n",
        "  \n",
        "  A **tokenizer** converts these *predicted tokens into human-readable text.* This stage may also involve further **post-processing** steps depending on the specific model and task (*e.g., adding timestamps*).\n",
        "\n",
        "\n",
        "\n",
        "## Using the ASR Pipeline:\n",
        "To use the **ASR pipeline**, one can simply `import the pipeline` function from transformers and specify the task as `\"automatic-speech-recognition\"`. A default model will be used if none is specified, or a specific model from the **Hugging Face Hub** can be provided."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmEpjWk9NiaF",
        "outputId": "1a9d2249-89ec-4626-86d8-063b60f8eef7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.2\n",
            "    Uninstalling transformers-4.56.2:\n",
            "      Successfully uninstalled transformers-4.56.2\n",
            "Successfully installed transformers-4.57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bevoIaNrAsuf"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I6lNBK2oD4VT"
      },
      "outputs": [],
      "source": [
        "task = \"automatic-speech-recognition\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# facebook/wav2vec2-base-960h\n",
        " -is an **automatic speech recognition (ASR)** model developed by Facebook (now Meta AI). It is a variant of the **Wav2Vec2** framework, known for its ability to learn powerful representations from speech audio alone.\n",
        "\n",
        "## Key features\n",
        "- Purpose: The model's primary function is to transcribe English audio into text.\n",
        "- Architecture: It is a \"base\" model, meaning it is the smaller version compared to the \"large\" variant. It uses a Transformer-based architecture.\n",
        "- Training data:\n",
        "  - Pre-training: Like other Wav2Vec models, it was initially trained in a self-supervised manner on a large amount of unlabeled audio data.\n",
        "  - Fine-tuning: It was then fine-tuned on 960 hours of labeled speech data from the Librispeech dataset. This final step adapts the model for the ASR task. The \"960h\" in its name specifically refers to this fine-tuning on 960 hours of Librispeech data.\n",
        "- Input requirements: For optimal performance, the model requires audio that has been sampled at a rate of 16kHz.\n",
        "- Performance: The Wav2Vec2 framework, including this base model, is known for achieving state-of-the-art results even when trained with limited labeled data.\n",
        "- Platform: The model is widely available on the Hugging Face model hub, making it easy for developers to download, use, and further fine-tune for specific applications."
      ],
      "metadata": {
        "id": "7GDO2uIBQ2gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How it works\n",
        "The model processes speech in two main stages:\n",
        "1. Feature encoding: The model's convolutional feature encoder processes raw audio waveforms to create a sequence of latent speech representations.\n",
        "\n",
        "2. Contextualization: These representations are then fed into a Transformer model, which contextualizes them by masking specific time steps and solving a contrastive task. This forces the model to learn the underlying structure of speech.\n",
        "\n",
        "## Use cases\n",
        "The model is suitable for a variety of speech-to-text applications, such as:\n",
        "- Transcribing audio files from podcasts or meetings\n",
        "- Building components for voice-controlled devices\n",
        "- Assisting people with disabilities who cannot use a keyboard"
      ],
      "metadata": {
        "id": "1jC60nl7Rkje"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4t6XwcMiDyjS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378,
          "referenced_widgets": [
            "898240ca8468402e86b393383138b455",
            "0aaa4ed756c14f02b94614ccaddda905",
            "00ad850191844d01a269fd1f8b85891d",
            "c6877256081043e8b69b106150382a9f",
            "478980b432fc4b489d4135c26c058135",
            "f5d9605630c946bbb3fde0dfc63eaa8f",
            "827fc5e2c2d848c0b84d606dce81db63",
            "2ec64d9f08d0424a83e111c763ece491",
            "382eac00371841bb87d8e7464c4e7738",
            "adafef87faa74aa58f34f9a0b62f76f6",
            "17de1905b7174a638a0157be93ef6acc",
            "8f951dd0c19a4d009fceab4bc661ad50",
            "7ad0633f80bb486baa706d9fe4e920e4",
            "5f70e6119cc940c6af3ccd0850657e40",
            "ccf686843f7b4838bce9328abc3a718d",
            "b04fef672f0947e7b268aa1badf9b240",
            "2c65a2f5ee094eb2bd55cbabaed1e29b",
            "124b00112b6748ca815422632ab91c9d",
            "7ed6f99a36b647b5a342b6bf2bc20eb5",
            "743dddb051be4f98a39aa917f793fc16",
            "c8809ac86ae44af4bdac9bf4e7194220",
            "9f9d4edadfd84e9fa275fbc0b43c9393",
            "69dee949330543bbb61474ee54b37926",
            "59611fb10bea4ee2842c53d5949cbb36",
            "eddd2e08e1cf46f4b2d0cf1b6bd790e3",
            "87293cee76fb461db701a14d1eb907bd",
            "48f9a116ce924c34b7125cc2c82daa38",
            "96b9adc54a8e406d9b1eb6d149646376",
            "a7b2b22dc73c42089cb53577d5ce7424",
            "ecaba37883534538a83cee19f2320241",
            "86d786b9b70a4c139778427c7692475a",
            "21e5c2d2edd04428b51a45748ccf6c0c",
            "5a0cb9b638f1429ba715022a05427f08",
            "daf2712aa692441fac15eb5dc2e9e860",
            "ea5cfc7fc78546a0a4a9dd286a697672",
            "10330c61f7e14a4ea9bf26474dd408fa",
            "1609c9acf6614535b8bd8b081dbcc0b7",
            "24b1e6aa292248fea54d7423b96e1807",
            "948a579afcd347eb92f66e5da5eefad2",
            "1c120037b9854884a78850f937251943",
            "7d4e61817a8d47a98810a5bb8d1d4b33",
            "ea9fe137dfd340568325e34c3a3ada43",
            "79dd9493e5b34be0b251f6982f7f1bf7",
            "fa19e47db05c4cc28189d635f325581c",
            "daac1ecd8ed04e81adda48cec7546805",
            "dbb20ef6157d43a4a67e2cbb952daddd",
            "5af88c755fc945ac933b0837958b63bc",
            "d6e40d3e91e3423489b5ef532f03bffc",
            "ee97655731dc481cafe82779baf64dc2",
            "73032d2d26ad428a821bcb01905a44e6",
            "60c3115b649f4e5daf2ece813d1fbaf4",
            "8ffba01fb9e84c04a69daf446a3d102b",
            "7d2d79c75a644ae8af70eab2f10e23c3",
            "f5938d61c5ae4fbe8c29ea63d78f0af1",
            "b4f88037dc1d4b069c949467544eab23",
            "3534be8cd517404ba74bca4627880a39",
            "143a984bea3e4328b6e60613fce057e3",
            "4c5f98c5fee54ad3ac5a03c3da1f92f2",
            "065d5bcc07334358a88f4ccf6cd8a3c5",
            "b73d7f9af7344fd89fdd2575c414e9a6",
            "bc0474f2652c43f5a0ffbcb6c0dd6526",
            "e53ec1d06d044c04a79a82c0e2c624d0",
            "4eaf5301e951499c9e9f7711887bec83",
            "381dbdb837104d08b861148a28f6ace0",
            "e2173378bf2b4cdaab83d614778eaad7",
            "b597c88ee84644ebb758b3ce4a7a3c31"
          ]
        },
        "outputId": "68c2033a-4f4f-4ccb-e177-4216f9bfc2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "898240ca8468402e86b393383138b455"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f951dd0c19a4d009fceab4bc661ad50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69dee949330543bbb61474ee54b37926"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daf2712aa692441fac15eb5dc2e9e860"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daac1ecd8ed04e81adda48cec7546805"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3534be8cd517404ba74bca4627880a39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_fb_wav2vec2_base_960h = \"facebook/wav2vec2-base-960h\"\n",
        "source_1 = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
        "asr_transcribe = pipeline(task, model=model_fb_wav2vec2_base_960h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bAMVdEjrFKJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4509224-a590-4cd0-fc8d-d8fe9b508d73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "asr_transcribe(inputs=source_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# openai/whisper-large-v3\n",
        "is a state-of-the-art, **multilingual automatic speech recognition (ASR)** and **speech translation model** developed by **OpenAI**. It excels at *transcribing and translating audio with high accuracy*, even in challenging conditions like *background noise and accents*.\n",
        "\n",
        "Trained on over **5 million hours** of labeled audio data, this model provides robust performance for various applications, including short and long-form transcription, and can be used via APIs or run locally.\n",
        "\n",
        "## Key Features\n",
        "- **High Accuracy: Whisper Large v3** offers improved accuracy over previous versions, with *reduced word error rates (WER) by 10-20%.*\n",
        "- **Multilingual Support:** It supports **99 languages**, enabling both **transcription and translation** for diverse linguistic inputs.\n",
        "\n",
        "- **Zero-Shot Generalization:** The model generalizes well to many datasets and domains, even without fine-tuning.\n",
        "\n",
        "- **Robustness:** It is highly robust to various audio conditions, including *background noise, accents, and technical jargon.*\n",
        "\n",
        "- **Versatile Applications:** It can be used for various applications, from short audio clips to sequential or chunked long-form audio transcription and translation.\n"
      ],
      "metadata": {
        "id": "qdqZXrNUJ0fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How It Works\n",
        "\n",
        "* **Input:** The model takes an *audio file as input*.\n",
        "\n",
        "* **Processing:** It utilizes its advanced architecture and training on *massive multilingual and multitask datasets* to process the audio.\n",
        "\n",
        "* **Output:** It outputs the *transcribed text of the audio* and can also translate it into another language.\n",
        "\n",
        "\n",
        "## Use Cases\n",
        "\n",
        "- **Transcription Services:** *Transcribing meetings, interviews, and other audio content.*\n",
        "\n",
        "- **Content Localization:** *Translating spoken content into different languages.*\n",
        "\n",
        "- **Accessibility:** *Providing accurate captions for videos and audio.*\n",
        "\n",
        "- **Voice Assistants:** *Enabling more robust and accurate speech understanding for voice-controlled devices.*"
      ],
      "metadata": {
        "id": "3jq2HAJiKAiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "53a483c94d7c41c795c218135c8cabef",
            "be1939304f1b4450baf389386529b73a",
            "2c27c3f1270b4311817719d91430d99b",
            "13897cc1aaec4971b0004052277e1c8c",
            "21df5de8f7044c03ad60b9bc50df8742",
            "2f0dfb4e413b4fe28d9d805d8f7f99f6",
            "6838449b5eba45da97a8e00aaa6ad25d",
            "7a065db79c4e4c2b89381929d45e6bd9",
            "f88eae4be46241a88707cf531ea63028",
            "43258a2ea9834f10ae080e93492a6ae8",
            "b0f753daecf04c1ea79a928058fb55ad",
            "7195b443f6604835ab4275d641366fc9",
            "d29c458df31143cba1f285c83df08793",
            "385dd40f0fb14d1c8384be956e51c628",
            "c8768ea6351a4175a65f296ea06c5c74",
            "6442d7483e1d4e77be4dcd384b5111fb",
            "30c82e592ba64957889bea2ad8f12215",
            "0cd15d25557b4caba5da78cad5598062",
            "cafab066289c4ba2ae34b9854bedec4b",
            "1f6a6be8685b466f9f37dbf4598594ad",
            "59d18ec1b63d464fb8378edac282b568",
            "b3b20adb151b478285ac53697327f564",
            "d743174d9a134405bb0550734e2e8d5b",
            "13564315f55148178db733792274ede2",
            "aa4551a904244b79bfb7e61712de5406",
            "6804682ef74f4b26bf98aec0773f41b1",
            "db2abadab11d464282249cb7e623d94d",
            "ef168a24581d472b8980f8f0112c886b",
            "2aeace35ac8b4af0a8d67838f8977ea5",
            "41084189fc29415d93d67bff2df12290",
            "e01073a26fdf4fc09327f3bc4d75e3f0",
            "708c72c56a16489f92e7749f5443a326",
            "0deb52816d3f4429a9cd5ea4b47c3db5",
            "c62f9d3142db4002a2accbb23c9c313a",
            "2433726c99b04870936b40b2d442f096",
            "feb6256efbfa42429573b945b8174d7b",
            "1bd970af0a18475694a4ff2294fb0ade",
            "6a8046a4f4d0455098944f5bddc67b29",
            "1deaf31a4947422aaf4dff489fa75355",
            "c21926eb449d4633bd846c9cd0af3005",
            "3d65dba639e14e97962a9f6d5fb350d1",
            "fdbd6c3034db4d7b9e49917682f8146d",
            "54c6909c0a9c4ecba0f808bcfb3be180",
            "e30857b7947e4adb87a487a31d36bbfe",
            "5ee854c5664c4c8b9f1740160590d839",
            "2919e0b9f7ba459796933cf84d408553",
            "4ab508110ea340b482afde75825aa142",
            "d60a318c91fa4010b98ee27f59386388",
            "fb9491a6edd441dcab7833928aa77941",
            "69e5df5c005c48728a65eed9d6d52bea",
            "a370ae69f4e144eeb6f449bee07dbb39",
            "c532676fb7ba45cba77b7834fc7b16a8",
            "7b9a686667af4c73b5fab8d3029fc71b",
            "87aa36326c384dc182f40c2bff3ddae1",
            "daf97d9e378047548876157cea7b318e",
            "e7090286757e470db679e0065147449a",
            "92fc5479164b47e1a5d4e76758a04b3f",
            "bad91eee86c2442ebdafc0433e715548",
            "2f28e794ef3345caa05b7fa4e2fe4670",
            "5af6fc2bc70144828b321c9eb36092a7",
            "a47364721e6649b99a3578799e7186ed",
            "48f1ff4d041a4a7fb1643923530f256a",
            "b49c1d4ca2f042f59b80a8c14a4df101",
            "f527fc8c2b96488b99282ef3819ab2fc",
            "6c4a398fcedd4ead86b49d5da4a312bd",
            "f13b3d9c1fab430fb7ca7069ef091365",
            "ecb9d2cf5d64418c98654a9b0198b428",
            "5a08f97592014996bd91560b6127ce3f",
            "16e8977001404db88fd21331eec7f193",
            "1dbd5f41076d4becb34f0f9c8152fe87",
            "2ea63619784348a48ce1ccd8c072f138",
            "fc3bd715584341f98429cf5af9c7271a",
            "cfce06005ea84b2c9c94be4f880cd515",
            "e80e8402c0e245d58b2395bb0945030b",
            "f6384a17de5244acab60b6c470fb0022",
            "45f1cb09f89646ffa41a1f74b59155ad",
            "fab369b0c7724ad1b7c587c0c1f3d7a1",
            "5a2ff104153340fb9158faaa01001db8",
            "006ef267c67746b7994177bb3e1fba2b",
            "a88d2009d8b346c0aa634cbb1458924d",
            "1ceca89dd90d4547b2be2e05b7a9af82",
            "d3faed4cbaf2489e8d82088d9a4bb940",
            "13c03167633d4c768bd2779aebd3b198",
            "5515ba860b8048d59e8abf6cea4c488c",
            "8d63e995fb484ea8812baae243540f5e",
            "b7edb06fd6e84a3fb52f5a0825225965",
            "08a81dd20b0d4ee08aa50caa9e3aac5d",
            "29d998f9b1ee4d8fbba918f6c0522c6c",
            "82e91b731fc04dd9a2fd5bd9f4dc44a8",
            "9a808abfa67e41ef93e420d3a4b59ce0",
            "08ec8af9fd3445af9797f2201ca49ff2",
            "2ebbdafcacae475da2a5eaf4845ce367",
            "214f98ef98c34d17ac578bead9ff55f7",
            "940d1f2b6ac243d59c9da3c631d86668",
            "a4f15547918d4c089ac806aee2315b9d",
            "3bb70e41d5884000b79b2bd6faff3578",
            "856f05ebf96f43f2a6049a7f8fe7c228",
            "5f9dfb5ee3ca47628170725c93d649c0",
            "f96da767f6674b1680ba6c65360074e6",
            "398573c4f0d8453aba1e6425b5ab1119",
            "8205a903ebf9436c9026ada7d6582b5c",
            "41aa78a48de04b25b4971cdd6ea21ea8",
            "d26fe3cfae5446eb8a48a176f5f7e076",
            "198bb5b0451647818f4e7273ea848294",
            "e00724c172184299af824338708572d2",
            "4e327f54332a445e8054e584819baa2d",
            "10e72d23993146c590f47549e1ca3538",
            "5c60d45e13d44078a25c9282c1574185",
            "72949c71238e4f128bd0681fdf2404f3",
            "040ea9a1d93047c08ca0ef62827b66d7",
            "a71d5cea65d54ba193ecb5d8ad5e1885",
            "d3a7129839cd48d498b30f50a870538f",
            "51b66d46ab004b6da8ecf9eb8378e87d",
            "ce96d205d4334255980188afebbc5ca2",
            "e55584fa900d43849dafa5a662141cc0",
            "57d6c5e22cca4b36b56fde1d982693fc",
            "37c013eee537449eb22109a10d04e03d",
            "29ce368335ef490c98b790627186f985",
            "77d4dec93ecc4f138ca8b3c692ffd4b3",
            "f1edbc11ed1c4f049c2b63fa8340df53",
            "3563398ab47f4c02a4c05b4179ae42a8"
          ]
        },
        "id": "BMtKqluIIC7Z",
        "outputId": "72a37864-9b2a-486c-d61d-2bb884bfba5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53a483c94d7c41c795c218135c8cabef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7195b443f6604835ab4275d641366fc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d743174d9a134405bb0550734e2e8d5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c62f9d3142db4002a2accbb23c9c313a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ee854c5664c4c8b9f1740160590d839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7090286757e470db679e0065147449a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecb9d2cf5d64418c98654a9b0198b428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normalizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a2ff104153340fb9158faaa01001db8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82e91b731fc04dd9a2fd5bd9f4dc44a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "398573c4f0d8453aba1e6425b5ab1119"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a71d5cea65d54ba193ecb5d8ad5e1885"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_openai_whisper_large_v3 =\"openai/whisper-large-v3\"\n",
        "# directly uploading from google colab\n",
        "source_2 = \"Aasan Nahin Yahan.mp3\"\n",
        "asr_transcribe = pipeline(task, model=model_openai_whisper_large_v3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asr_transcribe(inputs=source_2, return_timestamps=True, language='en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Sw8t0JOK2M",
        "outputId": "ce734e98-b597-4b88-ce79-ae7140576e21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' üéµ Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, Oh, oh, oh, oh, oh, oh, oh Bato se aage, wado se aage, dekho zara tum kabhi Yeh toh hai shola, yeh hai chingari, yeh hai jawag bhi Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, Jismon ke peechhe bhaage ho phirte Uthro kabhi rooh me Hota kya aashik kya aashiki hai Hogi khamar tal kune Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh,',\n",
              " 'chunks': [{'timestamp': (0.0, 28.0), 'text': ' üéµ'},\n",
              "  {'timestamp': (28.0, 49.0),\n",
              "   'text': ' Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, Oh, oh, oh, oh, oh, oh, oh'},\n",
              "  {'timestamp': (58.0, 70.76),\n",
              "   'text': ' Bato se aage, wado se aage, dekho zara tum kabhi'},\n",
              "  {'timestamp': (70.76, 81.04),\n",
              "   'text': ' Yeh toh hai shola, yeh hai chingari, yeh hai jawag bhi'},\n",
              "  {'timestamp': (81.04, 100.64),\n",
              "   'text': ' Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, Jismon ke peechhe bhaage ho phirte'},\n",
              "  {'timestamp': (100.64, 103.98), 'text': ' Uthro kabhi rooh me'},\n",
              "  {'timestamp': (103.98, 110.94), 'text': ' Hota kya aashik kya aashiki hai'},\n",
              "  {'timestamp': (110.94, 114.0), 'text': ' Hogi khamar tal kune'},\n",
              "  {'timestamp': (140.94, None),\n",
              "   'text': ' Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh,'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# openai/whisper-small\n",
        " -is the multilingual version of **OpenAI's Whisper small-sized** model for **automatic speech recognition (ASR)** and speech translation. The model is available on the Hugging Face Hub for developers to use and fine-tune.\n",
        "\n",
        "## Core capabilities\n",
        "- **Automatic Speech Recognition (ASR):** Transcribes spoken language into written text in the same language as the audio.\n",
        "\n",
        "- **Speech translation:** Translates speech from a source language into English.\n",
        "\n",
        "- **Multilingual support:** Trained on a diverse dataset of **680,000 hours** of labeled audio data, it can process speech in **98 different languages.**\n",
        "Timestamp prediction: Predicts sequence-level timestamps for transcriptions.\n",
        "\n",
        "## Technical specifications\n",
        "- **Model size:** The \"small\" model has **244 million parameters**, balancing\n",
        "performance and efficiency.\n",
        "\n",
        "- **Model architecture:** Uses a **transformer-based encoder-decoder** architecture, also known as a **sequence-to-sequence model.**\n",
        "\n",
        "- **Data training:** Built using a large and diverse dataset, which makes it robust to accents, background noise, and technical language.\n",
        "\n",
        "- **Input processing:** Takes *audio input of up to* **30 seconds.** Longer audio can be transcribed by splitting it into smaller chunks using a chunking algorithm."
      ],
      "metadata": {
        "id": "uLbn-0waPHJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How it compares to other Whisper models\n",
        "\n",
        "Whisper models come in several sizes to accommodate different needs, balancing speed, accuracy, and computational requirements.\n",
        "\n",
        "- **Larger models (e.g., medium, large):** Offer higher accuracy but require more memory (VRAM) and are slower.\n",
        "- **Smaller models (e.g., tiny, base):** Are faster and more efficient for devices with limited resources, but are less accurate.\n",
        "- **Small model:** Offers a strong balance between high accuracy and efficient performance, making it suitable for applications that require good recognition quality without the need for the largest, most resource-intensive models.\n",
        "\n",
        "## Usage on Hugging Face\n",
        "- To use openai/whisper-small via the Hugging Face library, you pair the model with a **WhisperProcessor**. This processor handles both the conversion of raw audio into a format the model can use and the conversion of the model's output tokens back into readable tex"
      ],
      "metadata": {
        "id": "pweIltU7PmbX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56071f46",
        "outputId": "126df754-56e9-4d9c-ee3b-2ecf6fbbfaae"
      },
      "source": [
        "model_openai_whisper_small = \"openai/whisper-small\"\n",
        "source_3 = \"Mera Hua Ek Deewane Ki Deewaniyat.mp3\"\n",
        "asr_transcribe = pipeline(task, model=model_openai_whisper_small)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40c295fc",
        "outputId": "b2b67e10-d8eb-468b-ab11-943b7d726dd5"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create an ASR pipeline configured for translation to English\n",
        "translator = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\", language=\"en\")\n",
        "\n",
        "# The transcribed text from the previous step is in the output of cell W032W0CxgV5j\n",
        "# We don't need the transcribed text directly for this approach,\n",
        "# we will pass the audio source directly to the pipeline for translation.\n",
        "source_to_translate = source_3\n",
        "\n",
        "# Translate the audio\n",
        "translated_output = translator(source_to_translate, return_timestamps=True)\n",
        "\n",
        "# Display the translated text\n",
        "print(\"Translated Text:\")\n",
        "print(translated_output['text'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text:\n",
            " Music I have become better than you I have made a promise to you I have become better than you I have made a promise to you So, what do I want from this world? You have become mine from both the worlds You have fallen in love with me You have fallen in love with me I've fallen in love with you since you were so young Oh, I have never seen you like this before I have never seen you like this before I have seen you in the evening, I have seen you in the day I have seen you in the evening, I have seen you in the day I met you a hundred times, but there was something missing Why do I feel as if I have never lived before? You are mine, so I am yours What do I want from this world? Both of us have grown up and you have become mine We have fallen in love and you have become mine We have fallen in love and you have become mine We have fallen in love and you have become mine You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W032W0CxgV5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8a32e8-e9e4-4391-ce5b-2b710323ffb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' ŸÖŸÑ⁄©€í ÿ™ÿ¨ÿ≥€í ÿ®€Åÿ™ÿ± ÿÆŸàÿßŸÜ€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ÿÆŸàÿØ ÿ≥€í €ÅŸà ⁄Øÿ¶€í €Å€å⁄∫ €ÅŸÖ ÿ™€åÿ±€å ⁄©ÿ≥ŸÖ ⁄Ü⁄æŸà⁄©€í ÿ¨ÿ® ÿ≥€í ŸÖ€Å⁄©€í ÿ™ÿ¨ ÿ≥€í €ÅŸà ⁄Øÿ¶€í €Å€å⁄∫ €ÅŸÖ ÿ™€åÿ±€å ⁄©ÿ≥ŸÖ ÿ™Ÿà ÿ¨Ÿà ŸÖ€åÿ±ÿß ÿ™Ÿà ŸÖ€å⁄∫ ⁄Üÿß€ÅŸà⁄∫ ÿßÿ≥ ÿØŸÜ€åÿß ÿ≥€í ÿ®⁄æ€å ⁄©€åÿß ÿØŸàŸÜ ÿ¨€ÅÿßŸÜŸà⁄∫ ÿ≥€í ÿ®⁄ë⁄æ ⁄©€í ÿ™ŸàŸÖ€åÿ±€Å €ÅŸàÿß ÿßÿ¥⁄©ŸÖ ÿ¢ŸÜŸà Ÿæ€í ÿØ⁄ë⁄æ ⁄©€í ÿ™ŸàŸÖ€åÿ±€Å €ÅŸàÿß ŸÑÿß⁄©⁄æ ÿ®€ÅÿßŸÜŸà⁄∫ ÿ≥€í ŸÑ⁄ë⁄æ ⁄©€í ÿ™ŸàŸÖ€åÿ±€Å €ÅŸàÿß ÿßÿ™ŸÜ€å ÿ≤ŸàÿßŸÜŸà⁄∫ Ÿæ€í ⁄Ü⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß ÿßŸà ⁄©ÿ®⁄æ€å Ÿæ€ÅŸÑ€í ÿØ⁄æ⁄ë⁄©ÿß €Å€å ŸÜ€Å€å⁄∫ ÿ≥€åŸÜ€í ŸÖ€å⁄∫ ÿ¨ÿ≥ÿØÿ± ŸÖ€åÿ±€í ÿ™⁄æÿß €Å€å ŸÜ€Å€å⁄∫ ÿ¥ÿßŸÖ ÿ®⁄æ€å ÿ™⁄©€Å€å ÿØŸÜ ŸÖ€åÿ±€í ÿ™⁄©€Å€å ÿ™Ÿà ŸÖŸÑÿß ÿ¨ÿ≥ ÿß⁄Øÿß ÿ±€Å ⁄Ø€åÿß ŸÖ€å⁄∫ Ÿà€Å€å⁄∫ ÿ≥Ÿà ÿ¢ÿ¨ÿßÿ™€å ŸÖŸÑ€å ⁄©⁄Ü⁄æ ÿ™ÿ™⁄æ€å Ÿæÿ± ⁄©ŸÖ€å ⁄©€åŸà⁄∫ ŸÑ⁄Ø€í Ÿæ€ÅŸÑ€í ŸÖ€å⁄∫ ÿ¨€åÿ≥€í ÿ¨€åÿß €Å€å ŸÜ€Å€å⁄∫ ÿ™Ÿà ÿ¨Ÿà ŸÖ€åÿ±ÿß ÿ™Ÿà ŸÖ€å⁄∫ ⁄Üÿß€ÅŸà⁄∫ ÿßÿ≥ ÿØŸÜ€åÿß ÿ≥€í ÿ®⁄æ€å ⁄©€åÿß ÿØŸàŸÜ ÿ¨€ÅÿßŸÜŸà⁄∫ ÿ≥€í ÿ®⁄ë⁄æ⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß ÿßÿ¥⁄© ŸÖ€ÅÿßŸÜŸà⁄∫ Ÿæ€í ÿØ⁄æ⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß ŸÑÿß⁄©⁄æ ÿ®€ÅÿßŸÜŸà⁄∫ ÿ≥€í ŸÑ⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß ⁄©ÿ™ŸÜ€í ÿ≤ÿ®ÿßŸÜŸà⁄∫ Ÿæ€í ⁄Ü⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæ',\n",
              " 'chunks': [{'timestamp': (0.0, 5.0),\n",
              "   'text': ' ŸÖŸÑ⁄©€í ÿ™ÿ¨ÿ≥€í ÿ®€Åÿ™ÿ± ÿÆŸàÿßŸÜ€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ⁄©€í ÿ®ÿßÿ±€í ÿÆŸàÿØ ÿ≥€í €ÅŸà ⁄Øÿ¶€í €Å€å⁄∫ €ÅŸÖ'},\n",
              "  {'timestamp': (6.0, 11.0), 'text': ' ÿ™€åÿ±€å ⁄©ÿ≥ŸÖ ⁄Ü⁄æŸà⁄©€í ÿ¨ÿ® ÿ≥€í'},\n",
              "  {'timestamp': (12.0, 18.0), 'text': ' ŸÖ€Å⁄©€í ÿ™ÿ¨ ÿ≥€í €ÅŸà ⁄Øÿ¶€í €Å€å⁄∫ €ÅŸÖ'},\n",
              "  {'timestamp': (19.0, 27.0), 'text': ' ÿ™€åÿ±€å ⁄©ÿ≥ŸÖ ÿ™Ÿà ÿ¨Ÿà ŸÖ€åÿ±ÿß ÿ™Ÿà ŸÖ€å⁄∫ ⁄Üÿß€ÅŸà⁄∫'},\n",
              "  {'timestamp': (28.0, 27.0), 'text': ''},\n",
              "  {'timestamp': (34.0, 40.0),\n",
              "   'text': ' ÿßÿ≥ ÿØŸÜ€åÿß ÿ≥€í ÿ®⁄æ€å ⁄©€åÿß ÿØŸàŸÜ ÿ¨€ÅÿßŸÜŸà⁄∫ ÿ≥€í ÿ®⁄ë⁄æ ⁄©€í ÿ™ŸàŸÖ€åÿ±€Å €ÅŸàÿß'},\n",
              "  {'timestamp': (40.0, 46.0), 'text': ' ÿßÿ¥⁄©ŸÖ ÿ¢ŸÜŸà Ÿæ€í ÿØ⁄ë⁄æ ⁄©€í ÿ™ŸàŸÖ€åÿ±€Å €ÅŸàÿß'},\n",
              "  {'timestamp': (46.0, 53.0), 'text': ' ŸÑÿß⁄©⁄æ ÿ®€ÅÿßŸÜŸà⁄∫ ÿ≥€í ŸÑ⁄ë⁄æ ⁄©€í ÿ™ŸàŸÖ€åÿ±€Å €ÅŸàÿß'},\n",
              "  {'timestamp': (53.0, 59.0), 'text': ' ÿßÿ™ŸÜ€å ÿ≤ŸàÿßŸÜŸà⁄∫ Ÿæ€í ⁄Ü⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß'},\n",
              "  {'timestamp': (83.0, 104.36),\n",
              "   'text': ' ÿßŸà ⁄©ÿ®⁄æ€å Ÿæ€ÅŸÑ€í ÿØ⁄æ⁄ë⁄©ÿß €Å€å ŸÜ€Å€å⁄∫ ÿ≥€åŸÜ€í ŸÖ€å⁄∫ ÿ¨ÿ≥ÿØÿ± ŸÖ€åÿ±€í ÿ™⁄æÿß €Å€å ŸÜ€Å€å⁄∫ ÿ¥ÿßŸÖ ÿ®⁄æ€å ÿ™⁄©€Å€å ÿØŸÜ ŸÖ€åÿ±€í ÿ™⁄©€Å€å'},\n",
              "  {'timestamp': (104.36, 110.36), 'text': ' ÿ™Ÿà ŸÖŸÑÿß ÿ¨ÿ≥ ÿß⁄Øÿß ÿ±€Å ⁄Ø€åÿß ŸÖ€å⁄∫ Ÿà€Å€å⁄∫'},\n",
              "  {'timestamp': (110.36, 113.36), 'text': ' ÿ≥Ÿà ÿ¢ÿ¨ÿßÿ™€å ŸÖŸÑ€å'},\n",
              "  {'timestamp': (113.36, 116.36), 'text': ' ⁄©⁄Ü⁄æ ÿ™ÿ™⁄æ€å Ÿæÿ± ⁄©ŸÖ€å'},\n",
              "  {'timestamp': (116.36, 123.36),\n",
              "   'text': ' ⁄©€åŸà⁄∫ ŸÑ⁄Ø€í Ÿæ€ÅŸÑ€í ŸÖ€å⁄∫ ÿ¨€åÿ≥€í ÿ¨€åÿß €Å€å ŸÜ€Å€å⁄∫'},\n",
              "  {'timestamp': (123.36, 129.36), 'text': ' ÿ™Ÿà ÿ¨Ÿà ŸÖ€åÿ±ÿß ÿ™Ÿà ŸÖ€å⁄∫ ⁄Üÿß€ÅŸà⁄∫'},\n",
              "  {'timestamp': (129.36, 136.66), 'text': ' ÿßÿ≥ ÿØŸÜ€åÿß ÿ≥€í ÿ®⁄æ€å ⁄©€åÿß'},\n",
              "  {'timestamp': (136.66, 143.16), 'text': ' ÿØŸàŸÜ ÿ¨€ÅÿßŸÜŸà⁄∫ ÿ≥€í ÿ®⁄ë⁄æ⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß'},\n",
              "  {'timestamp': (143.16, 149.36), 'text': ' ÿßÿ¥⁄© ŸÖ€ÅÿßŸÜŸà⁄∫ Ÿæ€í ÿØ⁄æ⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß'},\n",
              "  {'timestamp': (149.36, 155.96), 'text': ' ŸÑÿß⁄©⁄æ ÿ®€ÅÿßŸÜŸà⁄∫ ÿ≥€í ŸÑ⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß'},\n",
              "  {'timestamp': (155.96, 162.08), 'text': ' ⁄©ÿ™ŸÜ€í ÿ≤ÿ®ÿßŸÜŸà⁄∫ Ÿæ€í ⁄Ü⁄ë⁄©€í ÿ™Ÿà ŸÖ€åÿ±ÿß €ÅŸàÿß'},\n",
              "  {'timestamp': (185.96, None),\n",
              "   'text': ' ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæŸÜ€í ÿ≥ÿßÿ™⁄æ ÿ≥⁄©ÿ™ÿß €Å€í ⁄©€Å ÿßŸæ'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "asr_transcribe(inputs=source_3, return_timestamps=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

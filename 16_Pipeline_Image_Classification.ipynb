{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face image classification\n",
        "-refers to the use of **pre-trained models** and tools provided by **Hugging Face** to assign a label or category to an image. It involves inputting an *image into a model that has been trained* to recognize patterns and features, with the output being the most likely **class** *the image belongs* to, such as **\"cat\" or \"dog\"**.\n",
        "\n",
        "**Hugging Face** provides the infrastructure and resources, including a model hub, libraries like **transformers**, and tools like **AutoTrain**, to perform this task efficiently.\n",
        "\n",
        "### How it works:\n",
        "1. **Pre-trained Models:** Hugging Face hosts a vast collection of *pre-trained models* for various tasks, including image classification. These models are trained on large datasets and are ready to be used or fine-tuned for specific needs.\n",
        "2. **Pipelines:** The easiest way to use these models is through the **Hugging Face Transformers library pipelines**, which abstract away much of the complexity.\n",
        "3. **Input:** You provide an image (*as an image file or local bytes*) to the pipeline.\n",
        "4. **Classification:** The model analyzes the image's pixel values and predicts the most probable class.\n",
        "5. **Output:** The result is typically a set of classes and associated confidence scores, indicating the likelihood of each prediction.\n"
      ],
      "metadata": {
        "id": "cVICzIJHov-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key aspects of Hugging Face's approach:\n",
        "- **Model Hub:** A central platform to find, share, and use pre-trained models, including vision transformers.\n",
        "- **AutoTrain:** A service that simplifies the process of training custom image classification models by uploading labeled example images.\n",
        "- **Libraries:** Transformers and Datasets libraries provide the core functionalities for data loading, model inference, and training.\n",
        "\n",
        "### Applications:\n",
        "- **Keyword classification:** Assigning a keyword to an image.\n",
        "- **Image search:** Organizing photo libraries.\n",
        "- **Medical image screening:** Helping to detect signs of disease.\n",
        "- **Crop health monitoring:** Assessing the health of crops."
      ],
      "metadata": {
        "id": "cwH8QEV4o3i4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ah290abPoisT"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task=\"image-classification\"\n",
        "model_name=\"google/vit-base-patch16-224\"\n",
        "image_classifier= pipeline(task, model=model_name )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "d332487fac674dd3a506d39036cbd2cb",
            "049dbc570ee9478fbfbef46393a8b0b2",
            "48c5c33fd0d943039e3f24766bf7485f",
            "a12c569bc3e84d359936b2f837c6f8f6",
            "e111cf39a7014d1b858b63c8e29f77c3",
            "148812950cc943fe8d7589697f89e89b",
            "f6aeb2769fed4c24907802f3c275254b",
            "d0245cfaf7034a44810994943af556f2",
            "5ea9e488d65e4f2b9f4b696099818f70",
            "527af7a3d82a44a283f53a9b92dcbdf8",
            "de5365be6f8b422483a67253c5d32461",
            "9fa6e4995edc449d83a5391689c6080b",
            "50523c08945d4811918d2740af70e607",
            "2da04bc9e09046afb6524c1def02851a",
            "d98643b43a094fe4a3ecce410451a90f",
            "de8713e4da544ce7b6298cfcab5557c4",
            "a33ab1054f9246c3bea527a7f902d106",
            "494a9f2bf27244b18aa2271a1caaf9f6",
            "919236b4954841fa9c2c37c0776c4b00",
            "03535d0000014b73bdf4051e6a97f911",
            "b0ce99fd766945a98ddec8dd594b28bb",
            "317c64ac83ac40998eb288af5889d22e",
            "19fb228e48f147109c142a18bbaa0761",
            "d1b299738f294ecd96441d64971239c2",
            "7c97dff03705419dadc9be66e11eaac7",
            "ba736981f768472fb8fd323f7f2e6966",
            "8ea4cc8b5b154f288d401d6410836a2a",
            "7eac5f530099410ba08e813b0a6215ba",
            "3299083232cf47da8075c51cac521d92",
            "81043278693a4d3ba18b59b549d36ce2",
            "45b660226ad54f2886a60309a652a0be",
            "25ca1f7ac61b4b8984bb215f501b1102",
            "4005e88177f14fea92fc9e6c08b944d1"
          ]
        },
        "id": "4u8p8Gg6p5gr",
        "outputId": "61acf7a7-369d-42b6-d6ac-59bb8c8a1ddc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d332487fac674dd3a506d39036cbd2cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fa6e4995edc449d83a5391689c6080b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19fb228e48f147109c142a18bbaa0761"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# directly upload from google colab workspace location\n",
        "image_path = \"Amit_Visa_2022.jpeg\"\n",
        "\n",
        "results = image_classifier(image_path)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_uZ38cRrWGS",
        "outputId": "3d4a193d-d34a-40a3-c4a6-5a3db5b5db21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'envelope', 'score': 0.7954962253570557},\n",
              " {'label': 'rubber eraser, rubber, pencil eraser',\n",
              "  'score': 0.04921957105398178},\n",
              " {'label': 'rule, ruler', 'score': 0.029204238206148148},\n",
              " {'label': 'binder, ring-binder', 'score': 0.014957639388740063},\n",
              " {'label': 'packet', 'score': 0.011321413330733776}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# directly upload from internet location\n",
        "image_path =\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
        "\n",
        "results = image_classifier(image_path)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwbpdZGSEw_6",
        "outputId": "2eafe317-2b31-4fe9-953e-b553e6acc85d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'lynx, catamount', 'score': 0.43349990248680115},\n",
              " {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
              "  'score': 0.03479622304439545},\n",
              " {'label': 'snow leopard, ounce, Panthera uncia',\n",
              "  'score': 0.032401926815509796},\n",
              " {'label': 'Egyptian cat', 'score': 0.023944783955812454},\n",
              " {'label': 'tiger cat', 'score': 0.02288925088942051}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Image Classification:\n",
        "Hugging Face also supports **zero-shot image classification**, where you can classify an image based on a list of **candidate labels** without explicit training for those specific classes."
      ],
      "metadata": {
        "id": "CarWbTf-vP10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task=\"zero-shot-image-classification\"\n",
        "model_name=\"openai/clip-vit-base-patch32\"\n",
        "image_classifier= pipeline(task, model=model_name )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "9eda199199904b4aac9a29e312e796be",
            "6c77d6b4621a4db994e7908619c96806",
            "d77d231d6bb9424497041cbc964222a9",
            "0998e1bab0b04e90a8c4d9ace46992ba",
            "8fa04a3954124997a64ee1b548c04eb6",
            "140d16f509c74716a91aea1af6509052",
            "29a9e497b1b84bd5915a4c8137ee317d",
            "f7cc8cdbbeaf4b2ea13f11b963370d2f",
            "3259e29841d44afc927e60150fc046a9",
            "b973321861a8483481206cc985501f11",
            "a1decc1294be4cac9632cd8debd3c136",
            "531df8c796b64965abf28d0b5bf0dcbe",
            "81990027d2414a71a4e53b2aa450c982",
            "a052a3ddb0e04ecaafbf074339be93b6",
            "62fdec6c0800434a8f7ab4220dda3f65",
            "895064097c5e40dcb1fa36afb0f10b9e",
            "4273fa643b7543b1961b0a252d2416fa",
            "9c71fa7a32f04ab0a5d73249bfa05f1e",
            "1bf8be0d8f94403083255de2c6b7323a",
            "797791aceb1f412a81df927db9db54d2",
            "e76195fe0b274a449854c0531e8430e4",
            "660b96d3644b4551bc34981a8bb5a568",
            "2bcc5bd47385489dbe0850174c5a9d80",
            "591e1957ba3b4b9bbf879f6b6f4720f6",
            "d32263efd4f740ebb02bbb7ca6ad62a6",
            "b4639b4b94ab4bcdb0cc05240ef03e27",
            "19f6250cc1ff4ba497fc7694e57a17e7",
            "25bc67d026b24e3cbe08b43efeb8f1d5",
            "adb8afc0dc5f4fbab8a8e0f6f9c78933",
            "f47f55f4ee1f4ce783b1ed48456bdd40",
            "01fbbba5a20f4f068ca003aaffd9510d",
            "3096f2fa361445d1a79b6d80b9205b8b",
            "9e55573814d54210b2a5a0bb4972d06a",
            "fd010280ba9f480bad49f1d408223220",
            "98f91a39cabc444897aa2a8ebb9f75eb",
            "29025a40809343a8a08cf6d553e7d8c0",
            "757af94e9d5342848080ed3f6540f4e4",
            "c358221218ea499c919088394499269b",
            "8acd14dd9f664f7786e104aee2e4419d",
            "ef6ac22ef4fa4431bea24b2926ff55d5",
            "2fb0e3c2235b4d67ac220f216e7ae7a7",
            "bfe910a62c3b4adb804dff1c9d1ea1c5",
            "da21becc5ed5482c9a8e93d03055541a",
            "c3715c7b386c45cea26061ae8337d6ff",
            "dc48b39abbee41efbd2a9c358dff350c",
            "cd2c2a7fe0ba43bc90c9130de32edbf1",
            "6aacb7308d2342d2bf845b0402f2f443",
            "f2ea21d31f9343dbb3930c8fec7d1fab",
            "bed20abdfc5d4635b234ac9a312d462c",
            "29bc895786e74b96af7cdfc4f38a13e9",
            "16d9c11b6e25439e9de267ae58f14c9c",
            "9c258828b69048cdbe1a70037c5a89bd",
            "cf57275745f14569b3f707bdec20ff73",
            "6cf94cb10ab84767b7a59372ecb9d60c",
            "56d4d163b9b44d048b9c2253b3549b14",
            "b88f118994664252a4ac3831383fa14b",
            "741d31b344e747728b812e537cce1fd2",
            "6ff9f744d2a344bd88530165fe1ef5e9",
            "0afae91c714649b28ee334421a9a1eb6",
            "a5cc61c577eb482094e5ef2c1f5de37b",
            "967ebe28684d4139800a0f2fbc40e7d2",
            "e6fd22f3c9c44eabb3ac9e38a0a0fdee",
            "e3aeec45970848cda0bdfb4852142f54",
            "beb10ce151fa4e9db0abe51f96318fda",
            "f0f63c3ee598488abe559232dbcf6e0d",
            "d9cf608b1c2d45ecb732b665173c2136",
            "b9233099ff89423bba6b7c8d8f672bc2",
            "c5a3ca4c5d0e4f918c2921a2733a1638",
            "de75f89258114ea98d45744cd6b0f08e",
            "f0d755c6aeef4d23b5dc2d8e5370f241",
            "8ad1f88bb8d54335afd417b20608e3cc",
            "90577991d603467c851f52788d7f5a72",
            "7363f0ad7adf42498128a96038c74534",
            "02659b722ebf497a99554e44ddf9b568",
            "6102cfea266540948058b5f34ec59e10",
            "95b12ef3d52642f697a7f6d31e016dcb",
            "438ac9d570d14c65aa436e68b76dff89",
            "6a971adbb4ea4da1835591beac005c92",
            "6b255a827f114ff7bc3462d642c59308",
            "2386940111f244fb9595e5198239b414",
            "1f3d8ae95d6d43bbb481d2f60c6e88be",
            "172c1e4c537244f5b5668fafb4faa1ca",
            "3e68cbfa11194a7bb5d952ff1e757412",
            "ed649b92c3364d26a6b038217c0a263c",
            "5a0e4308a37b48dab22d944606a2c71b",
            "382cfbd109264ab48fee61484073ef6a",
            "aafdf68cceff40cd8ef8917423f77bae",
            "b50f21c50306478d9c23cdb0e7bb371c",
            "71aeb87422764b3cb83f22c1f5006b71",
            "e1419037a5c8431588e6802f1b0a0f1a",
            "d88ec678f09e4ff3856b1867eb583f48",
            "3a8990a7097643cea1f264873b757cb9",
            "e2f1513b0eb84df0b34afa8f540f3eea",
            "9d0127eedf3f45aa9b7306f04207c69b",
            "6e94792885b94c0cbefb7a5eda8183f2",
            "5a4824939ad443efaa2d6eb98a115fde",
            "27a070218ba74be89fc916f13c498157",
            "9be06926c40d4ca5b66f8e06ddcba2d9",
            "61655d4ea7c04b94ac54f30fcbc08a89"
          ]
        },
        "id": "ePy3ysF3vbQh",
        "outputId": "f7881993-558d-4982-8398-e58988d184be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eda199199904b4aac9a29e312e796be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "531df8c796b64965abf28d0b5bf0dcbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bcc5bd47385489dbe0850174c5a9d80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd010280ba9f480bad49f1d408223220"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc48b39abbee41efbd2a9c358dff350c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b88f118994664252a4ac3831383fa14b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9233099ff89423bba6b7c8d8f672bc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a971adbb4ea4da1835591beac005c92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71aeb87422764b3cb83f22c1f5006b71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"Amit_Visa_2022.jpeg\"\n",
        "label = ['eVisa', 'passport', 'photo']\n",
        "results = image_classifier(image_path, candidate_labels=label)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "G1jXUuJwwlbh",
        "outputId": "75628a49-085b-47ff-a6a2-1884e2a72119"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9997572302818298, 'label': 'passport'},\n",
              " {'score': 0.00019685605366248637, 'label': 'eVisa'},\n",
              " {'score': 4.5930421038065106e-05, 'label': 'photo'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}

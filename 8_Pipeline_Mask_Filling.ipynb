{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mask filling\n",
        "in a **pipeline** refers to using a **natural language processing (NLP)** model, specifically a **fill-mask task model**, *to predict a missing word *(the **\"mask\"**) within a sentence.\n",
        "\n",
        "This is done by feeding the model a sentence with a placeholder like **`[MASK]`** or **`<mask>`**.\n",
        "\n",
        "The **pipeline** then *returns the most probable words to fill that gap,* based on the surrounding context. This technique, also known as a **cloze test**, is used for tasks like\n",
        "* language learning,\n",
        "* content generation, and\n",
        "* data augmentation."
      ],
      "metadata": {
        "id": "fhitn6MYcsk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How it works\n",
        "1. **Inputting a masked prompt:** You provide a sentence to the **pipeline** with a special token representing the masked word (e.g., *\"The capital of France is \"*).\n",
        "2. **Model prediction:** The **fill-mask model** analyzes the sentence and predicts the most likely word(s) to replace the mask token, based on its training.\n",
        "3. **Outputting results:** The **pipeline** returns the predicted word(s), along with their scores (probability), to fill the gap."
      ],
      "metadata": {
        "id": "KherwfLVc0U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples of Use Cases\n",
        "* **Language Learning:**\n",
        "Creating fill-in-the-blank exercises for students.\n",
        "* **Content Generation:**\n",
        "Assisting writers in completing sentences or suggesting words.\n",
        "* **Data Augmentation:**\n",
        "Generating more diverse training data for other **NLP** tasks by predicting masked words in existing sentences.\n",
        "* **Domain-Specific Tasks:**\n",
        "Training models on specific datasets (like medical research papers) to understand and fill in context-specific information."
      ],
      "metadata": {
        "id": "VJtSeV24c3jy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-jMdK3LV__o"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"Amit is the most dynamic, aggressive and intelligent AI <mask>.\", top_k=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSh49DGPe8DH",
        "outputId": "fe623c24-a3b5-4754-d07a-7626bdd34d98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.04801853001117706,\n",
              "  'token': 869,\n",
              "  'token_str': ' player',\n",
              "  'sequence': 'Amit is the most dynamic, aggressive and intelligent AI player.'},\n",
              " {'score': 0.04232377931475639,\n",
              "  'token': 36749,\n",
              "  'token_str': ' imaginable',\n",
              "  'sequence': 'Amit is the most dynamic, aggressive and intelligent AI imaginable.'},\n",
              " {'score': 0.04084310308098793,\n",
              "  'token': 655,\n",
              "  'token_str': ' ever',\n",
              "  'sequence': 'Amit is the most dynamic, aggressive and intelligent AI ever.'},\n",
              " {'score': 0.037871986627578735,\n",
              "  'token': 1984,\n",
              "  'token_str': ' candidate',\n",
              "  'sequence': 'Amit is the most dynamic, aggressive and intelligent AI candidate.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}
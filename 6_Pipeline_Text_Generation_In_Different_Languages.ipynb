{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**XGLM-564M** is a multilingual autoregressive language model (*with 564 million parameters*) trained on a balanced corpus of a diverse set of **30 languages totaling 500 billion sub-tokens.**\n",
        "\n",
        "It was introduced in the paper [`Few-shot Learning with Multilingual Language Models`](https://arxiv.org/abs/2112.10668)"
      ],
      "metadata": {
        "id": "1u-U657Z9IO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model='facebook/xglm-564M',\n",
        "    device=0, # set to 0 for GPU\n",
        "    )"
      ],
      "metadata": {
        "id": "ToxfiA3Q8SNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `max_new_tokens = 50 .. 200`         # how long you want the new text to be\n",
        "* `do_sample = True or False`          # True => creative, False => deterministic\n",
        "* `temperature = 0.2 .. 0.8`           # 0.2 conservative, 0.7 creative-but-controlled\n",
        "* `top_p = 0.85 .. 0.95`                # nucleus sampling (use with do_sample=True)\n",
        "* `top_k = 0 .. 50`                    # optional; 0 means disabled\n",
        "* `repetition_penalty = 1.05 .. 1.3`   # helps avoid loops\n",
        "* `no_repeat_ngram_size = 2 or 3`      # prevents n-gram repetition\n",
        "* `num_return_sequences = 1`\n"
      ],
      "metadata": {
        "id": "AXl7gVWHG_ix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHve3iNE0bXy",
        "outputId": "7ec81f86-a9a0-420e-9abb-1c54122a675e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "भारत एक महान देश है और हम इस पर गर्व करते हैं। लेकिन यह बात भी सच नहीं हो सकती कि हमारे पास बहुत कुछ करने का मौका होता रहा, जो आज तक किसी ने कभी न दिया था । अगर आप अपने जीवन में ऐसे अवसरों को प्राप्त करना चाहते हों तो आपको अपनी क्षमताओं के बारें मे सोचना होगा ताकि आपके लिए कोई ऐसा काम कर सके जिससे वह आपकी सफलता की ओर अग्रसर रहे. आइए जानते हैं उन 10 चीज़ो से कैसे आपका व्यक्तित्व बदल सकता हे:- 1. दृढ़ता 2. आत्मविश्वास 3. संतुष्टि 4. साहस 5. निश्चय 6. ईमानदारी 7. सम्मान 8. प्रेम 9. करुणा 10. समर्पण 11. आदर्श 12. विश्वास 13. अनुशासन 14. उत्साह 15. प्रतिबद्धता 16. एकाग्रता 17. सकारात्मक ऊर्जा 18. शांता 19. सहानुभूति 20. आशा 21. सद्भाव 22. ज्ञान 23. आनंद 24. खुशी 25. पवित्र बुद्धि 26. विनम्र 27. उदारता 28. अहिंसा 29. दया 30. सदाचार 31. सत्य 32. न्याय 33. सेवा\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La inteligencia artificial es una herramienta clave del análisis de datos y de la toma de decisiones, pero no es suficiente. Tiene que ser aplicada en el campo de la administración de datos, en el análisis de los resultados de los proyectos realizados en los cuales se ha visto involucrada.\n",
            "\n",
            "L'avenir de la technologie est de la technologie\n"
          ]
        }
      ],
      "source": [
        "# Hindi\n",
        "prompt = \"भारत एक महान देश है और\"\n",
        "\n",
        "out = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,            # limit new tokens only (do NOT pass max_length)\n",
        "    do_sample=False,\n",
        "    temperature=0.2,\n",
        "    top_p=0.92,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1.15,\n",
        "    no_repeat_ngram_size=3,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "print(out[0][\"generated_text\"])\n",
        "\n",
        "# print(generator(\"भारत एक महान देश है और\", max_new_tokens=256, do_sample=True, temperature=0.7, truncation=True)[0]['generated_text'])\n",
        "\n",
        "print()\n",
        "\n",
        "# Spanish\n",
        "print(generator(\"La inteligencia artificial es\", max_length=100, num_return_sequences=1)[0]['generated_text'])\n",
        "print()\n",
        "\n",
        "# French\n",
        "print(generator(\"L'avenir de la technologie est\", max_length=100, num_return_sequences=1)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does each parameter does?\n",
        "#### `max_length=200`\n",
        "Limits the total sequence length (**input tokens + generated tokens**). This is the older/ambiguous option and is being phased out — *it counts the prompt length too,* which is often surprising. For modern code prefer `max_new_tokens`.\n",
        "#\n",
        "#### `max_new_tokens=256`\n",
        "Limits only the **new tokens** the model will generate (*doesn’t count prompt tokens*). This is the clearer, recommended way to cap generation length. Do not pass both `max_length` and `max_new_tokens` at the same time — that can raise an error or cause confusing behavior.\n",
        "#\n",
        "#### `do_sample=True`\n",
        "Turn on **stochastic** sampling. If **False**, the model takes a deterministic approach (**greedy search or beam search**). **True** => outputs vary each run and can be more *“creative” but also more likely to produce gibberish* if not combined with sensible sampling controls.\n",
        "#\n",
        "#### `temperature=0.7`\n",
        "Controls randomness when sampling. Lower values (e.g. **0.2–0.5**) → more conservative / focused output. Higher (**>1.0**) → more random / creative. **0.7** is a common balance for fluent, slightly creative output.\n",
        "#\n",
        "#### `truncation=True`\n",
        "Tells the **tokenizer** to truncate long prompts to the model’s max input length. NOTE: `truncation=True` is a tokenizer argument — passing it to the **text-generation pipeline** sometimes doesn’t behave as you expect (historically some pipelines ignored it). Safer: explicitly tokenize with `tokenizer(..., truncation=True, max_length=...)` before generation or check `tokenizer.model_max_length`."
      ],
      "metadata": {
        "id": "FI5OzB20EWEa"
      }
    }
  ]
}
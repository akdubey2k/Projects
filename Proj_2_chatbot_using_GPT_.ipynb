{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2yrutAQ3JNE"
      },
      "source": [
        "# [Training your own Chatbot using GPT​](https://www.youtube.com/watch?v=DxygPxcfW_I)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwyFBkGt44rE"
      },
      "source": [
        "# [Transformers](https://pypi.org/project/transformers/)\n",
        "**Transformers** provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n",
        "\n",
        "These models can be applied on:\n",
        "\n",
        "📝 **Text**, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n",
        "\n",
        "🖼️ **Images**, for tasks like image classification, object detection, and segmentation.\n",
        "\n",
        "🗣️ **Audio**, for tasks like speech recognition and audio classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
      ],
      "metadata": {
        "id": "yQWTcD0_VTFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKPiZcDY2-lJ",
        "outputId": "3f92d3f8-dcf0-453d-a8f1-2888f25e0108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.30.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYt1nqV7gQM"
      },
      "source": [
        "# [torch 2.3.0](https://pypi.org/project/torch/)\n",
        "**PyTorch** is a Python package that provides two high-level features:\n",
        "\n",
        "* Tensor computation (like NumPy) with strong GPU acceleration\n",
        "* Deep neural networks built on a tape-based autograd system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFzcRqLM7Tu3",
        "outputId": "9cecc8ef-1799-43c7-8910-ea0039faaeb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91OdOX1x9mGs"
      },
      "source": [
        "# [python-docx](https://pypi.org/project/python-docx/)\n",
        "**python-docx** is a Python library for reading, creating, and updating Microsoft Word 2007+ (.docx) files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x_Uycfc9w90",
        "outputId": "de1b9eac-376b-4239-dee6-d8fb72fd88b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUyBqrTY-Ck8"
      },
      "source": [
        "# [PyPDF2](https://pypi.org/project/PyPDF2/)\n",
        "**PyPDF2** is a free and open-source pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files. It can also add custom data, viewing options, and passwords to PDF files. PyPDF2 can retrieve text and metadata from PDFs as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHaBSrjv-tOm",
        "outputId": "563020ee-548e-4a0b-b69a-c68654f5739b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSWHBJ5D1OHb"
      },
      "source": [
        "### Import general required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uLaJHtS41Qcy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzWbOnL2DbGc"
      },
      "source": [
        "# All the below classes are provided by the \"Hugging Face\" Transformers library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLZOUWbr3Qf-"
      },
      "source": [
        "## [class transformers.GPT2Tokenizer](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Tokenizer)\n",
        "* The `GPT2Tokenizer` class is used for tokenizing text data in a way that is compatible with the GPT-2 model.\n",
        "* Tokenization involves converting a string of text into a list of tokens (words or subwords) that can be processed by the model.\n",
        "* This step is crucial for preparing text data for tasks such as text generation, text completion, or any other NLP application involving GPT-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJohBOYd4uOB"
      },
      "source": [
        "## [class transformers.GPT2LMHeadModel](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2LMHeadModel)\n",
        "\n",
        "`GPT2LMHeadModel` stands for \"Language Modeling Head.\" This model includes a language modeling head on top of the GPT-2 architecture, which is specifically designed for generating text. It predicts the next token in a sequence, making it suitable for tasks like text completion, text generation, and other sequence prediction tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjbM9gvk_q02"
      },
      "source": [
        "## TextDataset\n",
        "The `TextDataset` class helps in loading and processing text data in a format suitable for training and evaluation with transformer models.\n",
        "\n",
        "1. **Tokenization**: Automatically tokenizes the input text using a specified tokenizer.\n",
        "2. **Encoding**: Converts the tokenized text into numerical format that models can process.\n",
        "3. **Handling Large Datasets**: Efficiently handles large text files by loading data in chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0MUrl_8Asnd"
      },
      "source": [
        "## [class transformers.DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)\n",
        "\n",
        "This collator handles the batching and padding of sequences to ensure that all input sequences in a batch have the same length, which is necessary for efficient GPU utilization.\n",
        "\n",
        "It can be used for both <u>causal language modeling</u> (**CLM**) and <u>masked language modeling</u> (**MLM**) tasks, depending on the configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S5wvnlT31TRs"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6XgvszDH7xW"
      },
      "source": [
        "## [class transformers.Trainer](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/trainer#transformers.Trainer)\n",
        "\n",
        "It is a high-level API that simplifies the process of training and evaluating transformer models. It provides a flexible and easy-to-use interface for fine-tuning and training models on custom datasets.\n",
        "\n",
        "Few key features:-\n",
        "1. **Fine-Tuning Pre-Trained Models**:\n",
        "   - **Quick Fine-Tuning**: Easily fine-tune pre-trained models (like BERT, GPT-2, RoBERTa) on your custom datasets for specific tasks such as text classification, named entity recognition, or text generation.\n",
        "\n",
        "2. **Evaluation and Metrics**:\n",
        "   - **Built-in Evaluation**: Automatically handles evaluation during and after training, providing metrics such as accuracy, precision, recall, and F1 score.\n",
        "   - **Custom Metrics**: Allows users to define and use custom evaluation metrics.\n",
        "\n",
        "3. **Distributed Training**:\n",
        "   - **Multi-GPU Training**: Supports training across multiple GPUs to accelerate the process.\n",
        "   - **Distributed Training**: Can handle training on multiple nodes in a distributed setting, making it suitable for large-scale training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE58GoZSJl1s"
      },
      "source": [
        "## [class transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.41.0/en/main_classes/trainer#transformers.TrainingArguments)\n",
        "\n",
        "The `TrainingArguments` class is used to define the training configuration and hyperparameters for training Transformer-based models.\n",
        "\n",
        "By importing `TrainingArguments`, you gain access to a class that lets you specify various parameters such as the number of epochs, learning rate, batch size, evaluation strategy, logging options, and more, which are crucial for training machine learning models effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7mlA1eRNJM49"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU430dos1W6K"
      },
      "source": [
        "# <font color='red'>Caution: Use only single GPT model at a time.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMdLulg2CHik"
      },
      "source": [
        "# [GPT-2](https://huggingface.co/openai-community/gpt2)\n",
        "Pretrained model on English language using a <u>causal language modeling (CLM)</u> objective.\n",
        "\n",
        "## Model description\n",
        "**GPT-2** is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
        "\n",
        "This is the <u>**smallest** version of GPT-2, with **124M**</u> parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zECNSKv4B3Y0",
        "outputId": "0c6a364f-6002-4326-d846-714f80c32e04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Use a pipeline as a high-level helper\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "'''\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mbENcHkjB5q8",
        "outputId": "a6b77599-fd4d-4672-9ea1-92547b4b56ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load model directly\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3OQnpeF2Dp_"
      },
      "source": [
        "# [GPT-2 Medium](https://huggingface.co/openai-community/gpt2-medium)\n",
        "## Model Description\n",
        "**GPT-2 Medium** is the **355M** parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a <u>causal language modeling (CLM)</u> objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xrh49HH93-dv",
        "outputId": "106a5789-a1e1-4fa0-a067-9edda4ab0cbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Use a pipeline as a high-level helper\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-generation\", model=\"openai-community/gpt2-medium\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "'''\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2-medium\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "E_TxAygT4FQH",
        "outputId": "3b5c4694-6b53-4046-87d6-f74a29ab658b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load model directly\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'''\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU0LViDh1hTV"
      },
      "source": [
        "# [GPT-2 Large](https://huggingface.co/openai-community/gpt2-large)\n",
        "## Model Description\n",
        "**GPT-2 Large** is the **774M** parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a <u>causal language modeling (CLM)</u> objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z827LJt-2DQC",
        "outputId": "abead136-6c22-408a-d8eb-a515cf2d9460"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Use a pipeline as a high-level helper\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-generation\", model=\"openai-community/gpt2-large\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'''\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2-large\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "n2_T2act2KXZ",
        "outputId": "5a6be096-1373-4f1c-a98a-101434905ca6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load model directly\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "'''\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPrzMfl10xtH"
      },
      "source": [
        "# [GPT-2 XL](https://huggingface.co/openai-community/gpt2-xl)\n",
        "## Model Description\n",
        "**GPT-2 XL** is the **1.5B** parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a <u>causal language modeling (CLM)</u> objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "90zZuz-B0Odx",
        "outputId": "d6c9eb6f-dd9a-4a6d-cbbe-22ddc698e9a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import pipeline, set_seed\\ngenerator = pipeline(\\'text-generation\\', model=\\'gpt2-xl\\')\\nset_seed(42)\\ngenerator(\"Hello, I\\'m a language model,\", max_length=30, num_return_sequences=5)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "'''\n",
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2-xl')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ERPpNAvaJ0f"
      },
      "source": [
        "# functions to read different file types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CXD-ztg5X8TY"
      },
      "outputs": [],
      "source": [
        "# read any number of pdf documents from the directory path and stored as a text file.\n",
        "def read_pdf(file_path):\n",
        "  with open(file_path, 'rb') as file:\n",
        "    pdf_reader = PdfReader(file)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      text += pdf_reader.pages[page_num].extract_text()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7ktjRIwsaMaR"
      },
      "outputs": [],
      "source": [
        "# read any number of word documents from the directory path and stored as a text file.\n",
        "def read_word(file_path):\n",
        "  doc = docx.Document(file_path)\n",
        "  text = \"\"\n",
        "  for paragraph in doc.paragraphs:\n",
        "    text += paragraph.text + \"\\n\"\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tjl-e_32bfnM"
      },
      "outputs": [],
      "source": [
        "# read any number of text documents from the directory path and stored as a text file.\n",
        "def read_text(file_path):\n",
        "  with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wT_0Yz7kXIFi"
      },
      "outputs": [],
      "source": [
        "# read all types of and any number of documents from the directory path and stored as text file.\n",
        "def read_documents_from_directory(directory):\n",
        "  combined_text = \"\"\n",
        "  for filename in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, filename)\n",
        "\n",
        "    if filename.endswith(\".pdf\"):\n",
        "      combined_text += read_pdf(file_path)\n",
        "    elif filename.endswith(\".docx\"):\n",
        "      combined_text += read_word(file_path)\n",
        "    elif filename.endswith(\".txt\"):\n",
        "      combined_text += read_text(file_path)\n",
        "  return combined_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMh2xql_bpaT"
      },
      "source": [
        "Train a chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4bkisKSNbstf"
      },
      "outputs": [],
      "source": [
        "def train_chatbot(directory, model_output_path, train_fraction=0.8):\n",
        "  # read documents from the directory\n",
        "  combined_text = read_documents_from_directory(directory)\n",
        "  # removing excess new line characters\n",
        "  combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()\n",
        "\n",
        "  # split the text into training and validation sets\n",
        "  split_index = int(train_fraction * len(combined_text))\n",
        "  train_text = combined_text[:split_index]\n",
        "  test_text = combined_text[split_index:]\n",
        "\n",
        "  # save training and validation data as text file\n",
        "  with open(\"train.txt\", \"w\") as f:\n",
        "    f.write(train_text)\n",
        "  with open(\"test.txt\", \"w\") as f:\n",
        "    f.write(test_text)\n",
        "\n",
        "  # save up the tokenizer and model\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # also try gpt2, gpt2-large, gpt2-medium, gpt2-xl\n",
        "  model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # also try gpt2, gpt2-large, gpt2-medium, gpt2-xl\n",
        "\n",
        "  # prepare the dataset\n",
        "  train_dataset = TextDataset(tokenizer=tokenizer, file_path='train.txt', block_size=128)\n",
        "  test_dataset = TextDataset(tokenizer=tokenizer, file_path='test.txt', block_size=128)\n",
        "  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "  # setup the training argument\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=model_output_path,\n",
        "      overwrite_output_dir=True,\n",
        "      per_device_train_batch_size=4,\n",
        "      per_device_eval_batch_size=4,\n",
        "      num_train_epochs=4,\n",
        "      save_steps=10_000,\n",
        "      save_total_limit=2,\n",
        "      logging_dir='./logs',\n",
        "  )\n",
        "\n",
        "  # train the model\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      data_collator=data_collator,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=test_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model(model_output_path)\n",
        "\n",
        "  # save the tokenizer\n",
        "  tokenizer.save_pretrained(model_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz115v-UmsgI"
      },
      "source": [
        "Use the implemented model and try to generate the text.\n",
        "\n",
        "The generate_response function takes a trained model, tokenizer, and a prompt string as input and generates a response using the GPT-2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='red'>Output Length Limitation:</font> This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
      ],
      "metadata": {
        "id": "zgIFTDTsflqO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "c9Om_EPhm0M6"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=1023):\n",
        "  inputs_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "  # create the attention mask and pad token ids\n",
        "  attention_mask = torch.ones_like(inputs_ids)\n",
        "  pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "  output = model.generate(\n",
        "      inputs_ids,\n",
        "      max_length=max_length,\n",
        "      num_return_sequences=1,\n",
        "      attention_mask=attention_mask,\n",
        "      pad_token_id=pad_token_id\n",
        "  )\n",
        "\n",
        "  return tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV84vvoETf6j",
        "outputId": "f6654416-b8c9-4de7-aade-db95590fe566"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5w-cAywDEKpZ"
      },
      "outputs": [],
      "source": [
        "# the main function to store all files which will be use in the program\n",
        "def main():\n",
        "  directory = \"/content/drive/MyDrive/Colab Notebooks/Project/\"\n",
        "  # /content/drive/MyDrive/Colab Notebooks/Project\n",
        "  model_output_path = \"/content/drive/MyDrive/Colab Notebooks/Project/\"\n",
        "\n",
        "  # Check if the directory exists\n",
        "  if not os.path.exists(directory):\n",
        "    raise FileNotFoundError(f\"Directory '{directory}' not found.\")\n",
        "\n",
        "  # Check if the user has access to the directory\n",
        "  if not os.access(directory, os.R_OK):\n",
        "    raise PermissionError(f\"User does not have read access to '{directory}'.\")\n",
        "\n",
        "  # train the chatbot\n",
        "  train_chatbot(directory, model_output_path)\n",
        "\n",
        "  # Load the fine-tuned model and tokenizer\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_output_path)\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)\n",
        "\n",
        "  # Test the chatbot\n",
        "  prompt = \"What is Jacquard machines are made of?\"  # Replace with your desired prompt\n",
        "  response = generate_response(model, tokenizer, prompt)\n",
        "  print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iiIVYQuZE_t1",
        "outputId": "7431ff0d-717f-47af-e705-c4e8bb94b5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='572' max='572' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [572/572 01:42, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.374400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: What is Jacquard machines are made of?\n",
            "Jacquard machines are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is\n",
            "used for the manufacture of the Jacquard cards, cards for the\n",
            "card-stamping machine, and the filling machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is used for the\n",
            "manufacturing of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is\n",
            "used for the manufacture of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is\n",
            "used for the manufacture of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is\n",
            "used for the manufacture of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is\n",
            "used for the manufacture of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety of materials, including\n",
            "fabrics, carpets, and fabrics. The Jacquard machine is\n",
            "used for the manufacture of the Jacquard cards, cards for the filling machine, and the filling\n",
            "machine.\n",
            "Jacquard cards are made of a variety\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}